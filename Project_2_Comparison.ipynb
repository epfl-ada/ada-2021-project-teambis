{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce69895e-c55b-41e2-a702-40a778061889",
   "metadata": {},
   "source": [
    "### Comparaison sémantique\n",
    "\n",
    "(Basé sur la technique du paper Sentence Similarity Techniques for Short vs Variable Length Text using Word Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc41f43-2d9f-4efc-8e67-a4690f9a443a",
   "metadata": {},
   "source": [
    "### Test avec tout 2020 (tjrs avec juste quelques expressions test, vu qu'on a pas choisi lesquelles on garde)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b925c-7bcd-4356-a62c-5cfd755528b1",
   "metadata": {
    "id": "5E6s11nPpS1S"
   },
   "source": [
    "#### Install and import every packages that will be necessary for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f07b8d-ab39-4471-a7d9-44165d6f0e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDFHjZ-DpMXi",
    "outputId": "5ac51208-9817-41c8-a3c9-d8830978e78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle-mixin in c:\\users\\flore\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\flore\\anaconda3\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\flore\\anaconda3\\lib\\site-packages (from pyarrow) (1.20.1)\n",
      "Requirement already satisfied: pathlib in c:\\users\\flore\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle-mixin\n",
    "!pip install pyarrow\n",
    "!pip install pathlib\n",
    "\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import bz2\n",
    "import json\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01a983-c9e2-42d6-a081-d94e6344461b",
   "metadata": {
    "id": "cx0MEzbZw5NM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a1e7c-e419-4f08-9a60-f4aadf8bee56",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d124d21-abbf-4b7a-931d-d7770505d76b",
   "metadata": {},
   "source": [
    "### Imports, et import du pretrained model qu'on utilise ( pour l'instant all-MiniLM-L6-v2 me semble le mieux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c837898-f1c0-4a46-bb9c-4ccf601a46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si besoin\n",
    "#pip install -U sentence-transformers\n",
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d380df6c-7b3f-4888-90ac-a486c148ab91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEXXUZjT4yN7",
    "outputId": "c638bc74-0576-40b7-b515-a8838a784bc1"
   },
   "outputs": [],
   "source": [
    "#NLP libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "#from gensim.models import FastText #pas utilisé finalement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2291205-3aec-4c9a-b769-eb55e972ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#model = SentenceTransformer(r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\pretrained_model') #pour si la ligne d'en dessous ne marche pas, telecharger le model en local et mettre le path\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #import of the pretrained model we will use\n",
    "\n",
    "\n",
    "# j'ai testé plysieurs models:\n",
    "\n",
    "# all-MiniLM-L6-v2: ça passe, treshold à 0.6 ?\n",
    "#'bert-base-nli-mean-tokens' : deprecated\n",
    "#all-mpnet-base-v2: ne se télécharge pas\n",
    "#multi-qa-mpnet-base-dot-v1: ne va pas, tous les scores sont trés haut meme pour 'truc contraires\n",
    "#multi-qa-distilbert-cos-v1: mouai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4be65-4f86-44b3-96bc-7ce1ec731f3d",
   "metadata": {},
   "source": [
    "#### create a dataframe de test with the expression we want to compare, and their score (remplacer aprés avec le vrai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3acc773-b09f-4e7e-9063-8b5ae2c2a890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expression</th>\n",
       "      <th>expression_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont know</td>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I suppose</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm sure</td>\n",
       "      <td>6.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I know</td>\n",
       "      <td>6.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     expression  expression_score\n",
       "0  I'm not sure              2.91\n",
       "1   I dont know              3.02\n",
       "2     I suppose              3.34\n",
       "3       I think              3.48\n",
       "4      I'm sure              6.02\n",
       "5        I know              6.45"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe with the ewpression we want to compare, and their score\n",
    "expressions = pd.DataFrame({'expression': [\"I'm not sure\", \"I dont know\", \"I suppose\", \"I think\", \"I'm sure\", \"I know\"], 'expression_score': [2.91, 3.02, 3.34, 3.48, 6.02, 6.45]})\n",
    "expressions.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b60e09-4faa-4ba0-8e9d-773c451e86f2",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0b38d-774f-41c5-ae4e-6f43aa45f605",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparaison des citations (morceau par morceau) aux expressions du paper, et création d'un dataframe avec qids, quote, cosine_similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce99926-7793-4d33-aeba-2fbcafd4d4f6",
   "metadata": {},
   "source": [
    "Je decoupe les citations. Pour chaque expression: je compare chaque morceau de la citation avec l'expression, ça donne un score de similarité pour chaque morceau: on prend le max (ou bien faire un teshold ? à voir).  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caa89118-8063-43ba-b3c1-999b3cbbbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\quotes-processed.json.bz2' #path florette\n",
    "\n",
    "i = 0\n",
    "\n",
    "with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=10, orient = 'records') as df_reader:\n",
    "    for quotes in df_reader:\n",
    "        #First, we separate the word and symbol using tokenisation. This will allow us to then reform n_grams with the desired number of words/symbol.\n",
    "        quotes['n_grams'] = quotes['quotation'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "        #Then reform chunk of n words\n",
    "        n = 4 #number in n_gram\n",
    "        quotes['n_grams'] = quotes['n_grams'].apply(lambda x: (list(\" \".join(x[i : i+n-1]) for i in range(len(x) - n + 1))))\n",
    "        \n",
    "        #coosine similarity\n",
    "        quotes['cosine_similarities'] = quotes['n_grams'].apply(lambda x: list(max(cosine_similarity([n_gram_embedded], [expression_embedded]) for n_gram_embedded in model.encode(x)).flatten()[0] for expression_embedded in model.encode(expressions.expression)))\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        if i >= 1 : break\n",
    "            \n",
    "            \n",
    "#df_reader = pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=100000, orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aee35e5-e60f-462e-b75b-3d7051c1af7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>quotation</th>\n",
       "      <th>size</th>\n",
       "      <th>n_grams</th>\n",
       "      <th>cosine_similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sue Myrick</td>\n",
       "      <td>[Q367796]</td>\n",
       "      <td>Department of Homeland Security was livid and ...</td>\n",
       "      <td>14</td>\n",
       "      <td>[Department of Homeland, of Homeland Security,...</td>\n",
       "      <td>[0.3032319, 0.3031956, 0.23602816, 0.22850059,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meghan King Edmonds</td>\n",
       "      <td>[Q20684375]</td>\n",
       "      <td>I met them when they just turned 4 and 7. They...</td>\n",
       "      <td>19</td>\n",
       "      <td>[I met them, met them when, them when they, wh...</td>\n",
       "      <td>[0.2303027, 0.2367206, 0.27510583, 0.29699588,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dexter Smith</td>\n",
       "      <td>[Q5268447]</td>\n",
       "      <td>The delay will have an impact on Slough but th...</td>\n",
       "      <td>37</td>\n",
       "      <td>[The delay will, delay will have, will have an...</td>\n",
       "      <td>[0.41248465, 0.34659547, 0.50099874, 0.6188506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barry Coppinger</td>\n",
       "      <td>[Q4864119]</td>\n",
       "      <td>The scheme treats addiction as an illness and ...</td>\n",
       "      <td>16</td>\n",
       "      <td>[The scheme treats, scheme treats addiction, t...</td>\n",
       "      <td>[0.27248415, 0.21348144, 0.23313306, 0.2357903...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben Carson</td>\n",
       "      <td>[Q816459]</td>\n",
       "      <td>These actions will allow households who have a...</td>\n",
       "      <td>27</td>\n",
       "      <td>[These actions will, actions will allow, will ...</td>\n",
       "      <td>[0.2834597, 0.28215024, 0.22757119, 0.18103966...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               speaker         qids  \\\n",
       "0           Sue Myrick    [Q367796]   \n",
       "1  Meghan King Edmonds  [Q20684375]   \n",
       "2         Dexter Smith   [Q5268447]   \n",
       "3      Barry Coppinger   [Q4864119]   \n",
       "4           Ben Carson    [Q816459]   \n",
       "\n",
       "                                           quotation  size  \\\n",
       "0  Department of Homeland Security was livid and ...    14   \n",
       "1  I met them when they just turned 4 and 7. They...    19   \n",
       "2  The delay will have an impact on Slough but th...    37   \n",
       "3  The scheme treats addiction as an illness and ...    16   \n",
       "4  These actions will allow households who have a...    27   \n",
       "\n",
       "                                             n_grams  \\\n",
       "0  [Department of Homeland, of Homeland Security,...   \n",
       "1  [I met them, met them when, them when they, wh...   \n",
       "2  [The delay will, delay will have, will have an...   \n",
       "3  [The scheme treats, scheme treats addiction, t...   \n",
       "4  [These actions will, actions will allow, will ...   \n",
       "\n",
       "                                 cosine_similarities  \n",
       "0  [0.3032319, 0.3031956, 0.23602816, 0.22850059,...  \n",
       "1  [0.2303027, 0.2367206, 0.27510583, 0.29699588,...  \n",
       "2  [0.41248465, 0.34659547, 0.50099874, 0.6188506...  \n",
       "3  [0.27248415, 0.21348144, 0.23313306, 0.2357903...  \n",
       "4  [0.2834597, 0.28215024, 0.22757119, 0.18103966...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796cd8b-4a32-49af-bee6-f19f6bb78e6e",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa42f72-af2f-46e8-bf0d-853ea06ca449",
   "metadata": {},
   "source": [
    "je voulais mettre les scores des differentes expressions dans differentes colonnes, mais j'y arrive pas. Comme je sais pas si ça ser plus pratique pour la suite tout dans une meme colonne ou dans differentes colonnes je laisse comma ça en attendant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f78df-7f94-4965-a460-e24b371ff0a4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0748d-442d-472e-894c-7d7999892040",
   "metadata": {},
   "source": [
    "## Probleme à résoudre:\n",
    "on va choisir un treshold pour l'aggregation des scores. Le probleme, c'est que là dans mon code on choisit le nombre de mots dans les n grams (n), on fait les n grams, puis ont fait la comparaison avec les expressions. Le truc c'est que on a des expressions de 2 à 5 mots qu'on compare avec des n grams tous de n mots, et ça pourrait se voir sur les score et la precision du trashold (genre idealement ça aurait peut etre été mieux d'adapter n à chaque expression, mais ça veut dire recalculer les ngrams pour chaque citations, j'ai peur que ça soit monstrueux et que ça rallong enormement...). Je ferais quelques test pour voir si ça gene pour le treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0cc65-5cb7-433f-ae4e-6c5fbef6be2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeaca46-7399-4d02-bd25-a517eb6e230b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49461b66-0213-462d-bb85-0869bb8fd9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5824c0b-85c2-44a3-ad1f-10e30f22becc",
   "metadata": {},
   "source": [
    "### Pour tester direct sur un petit dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403629b-19af-41f6-96d6-9b4770a91fa3",
   "metadata": {},
   "source": [
    "#### Extract a small part of the whole quotes file to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "504062fe-45c9-40c2-8ba2-c9a88bda071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract a small part of the whole file to test \n",
    "path = r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\quotes-processed.json.bz2' #path florette\n",
    "quotes = pd.DataFrame(columns=('qids','quotation'))\n",
    "i = 0\n",
    "with bz2.open(path, 'rb') as s_file:\n",
    "  for instance in s_file:\n",
    "    instance = json.loads(instance) \n",
    "    #print(instance)\n",
    "    #print(instance['quotation'])\n",
    "    i = i + 1\n",
    "    quotes = quotes.append({'qids': instance['qids'], 'quotation': instance['quotation']}, ignore_index=True)\n",
    "    #print(i)\n",
    "    if i == 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30a95e-5ba8-4dc1-82f7-8dd4f74dc5dd",
   "metadata": {},
   "source": [
    "#### append some quotation for testing the efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c638384-195f-4e74-bdf4-82361e7d861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = quotes.append({'qids': 'Qtest1', 'quotation': 'I know it is a fool'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest2', 'quotation': 'I dont know if it is a fool'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest3', 'quotation': 'I am sure it is a fool'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest4', 'quotation': 'I am not sure if it is a fool'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest5', 'quotation': 'I dont eat an apple'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest6', 'quotation': 'I pretty sure if it is a fool'}, ignore_index=True)\n",
    "quotes = quotes.append({'qids': 'Qtest7', 'quotation': 'I was sure if it is a fool'}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde25f-f42d-4e19-83f8-28f66615fcf7",
   "metadata": {},
   "source": [
    "#### Create the n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23518161-9271-45b0-a196-66efe8f2fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we separate the word and symbol using tokenisation. This will allow us to then reform n_grams with the desired number of words/symbol.\n",
    "        quotes['n_grams'] = quotes['quotation'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "        #Then reform chunk of n words\n",
    "        n = 4 #number in n_gram\n",
    "        quotes['n_grams'] = quotes['n_grams'].apply(lambda x: (list(\" \".join(x[i : i+n-1]) for i in range(len(x) - n + 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7992d-7229-4528-9579-fe4c787c5f62",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55cb27-feb8-4755-a07d-13fbe689056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coosine similarity\n",
    "        quotes['cosine_similarities'] = quotes['n_grams'].apply(lambda x: list(max(cosine_similarity([n_gram_embedded], [expression_embedded]) for n_gram_embedded in model.encode(x)).flatten()[0] for expression_embedded in model.encode(expressions.expression)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175d169-5c41-4b89-89c0-0054bef1ca3f",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
