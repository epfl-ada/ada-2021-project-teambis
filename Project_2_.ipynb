{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_fe4oxWT6p"
      },
      "source": [
        "# Applied data analysis\n",
        "## Self-confidence through quotations:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the environment properly "
      ],
      "metadata": {
        "id": "YK_J_Eag8RRI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnXvjWR9NWvZ"
      },
      "source": [
        "### Mount the Google Drive in order to access to the files which are located on our drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEcIlRwfWY4C",
        "outputId": "e596769c-3b42-4e13-a264-e9d4748e36ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6s11nPpS1S"
      },
      "source": [
        "### Install and import every packages that will be necessary for the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDFHjZ-DpMXi",
        "outputId": "e95c5ec0-2b32-4fe8-c612-3481c3fcb3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pickle-mixin\n",
            "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6008 sha256=93cfd4ab62e0fb564e3c236fa419e76a203220ca2c85212cd6839651cdb750c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/70/0b/673e09a7ed429660d22352a1b117b4f616a8fc054bdd7eb157\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin\n",
            "Successfully installed pickle-mixin-1.0.2\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pickle-mixin\n",
        "!pip install pyarrow\n",
        "!pip install pathlib\n",
        "\n",
        "\n",
        "import pyarrow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import bz2\n",
        "import json\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jImkrItnY9wA"
      },
      "source": [
        "# Preprocessing of the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsgGgvy_Qte3"
      },
      "outputs": [],
      "source": [
        "# The following function allows to only keep the attributes when a single evalue is given. \n",
        "#The function replaces array of size 1 by its first element\n",
        "#Indeed, some attributes are associated with an array of value, to make sure that the value we will use is the correct one, \n",
        "#we simple get rid of multiple valued element by replacing them with a 'Nan' value.\n",
        "\n",
        "def rem_mult(array):\n",
        "  if isinstance(array, np.ndarray):\n",
        "    if array.size == 1 :\n",
        "      array = array[0]\n",
        "    else :\n",
        "      array = np.nan\n",
        "  return array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKUy29h68N0J"
      },
      "source": [
        "#### Extracting the relevant data from the original files and saving them in a new file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "t7BjB7AW9qst",
        "outputId": "4d64fc62-ea64-4bc0-af68-b4deeae9a7c6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-16adeacbb192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquotes_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bz2.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mbz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"t\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "path_to_file = '/content/drive/MyDrive/ADAprojet2021/Quotebank/quotes-2020.json.bz2' \n",
        "path_to_out = '/content/drive/MyDrive/ADAprojet2021/quotes-2020-clean.json.bz2'\n",
        "\n",
        "\n",
        "quotes_size =[]\n",
        "with bz2.open(path_to_out, 'wb') as d_file:\n",
        "    with bz2.open(path_to_file, 'rb') as s_file:\n",
        "        for instance in s_file:\n",
        "            instance = json.loads(instance) # loading a sample as a dictionnary\n",
        "            if (instance['speaker'] != 'None' and len(instance['qids']) == 1):\n",
        "              #Get rid of the quotes associated with no speaker as well as \n",
        "              # quotes associated with multiple qids\n",
        "                instance_out = {}\n",
        "                instance_out['speaker'] = instance['speaker']\n",
        "                instance_out['qids'] = instance['qids']\n",
        "                #remove [ and ] \n",
        "                instance_out['quotation'] = instance['quotation'].replace(\"[ \", \"\").replace(\" ]\", \"\")      \n",
        "                size = len(instance_out['quotation'].split())\n",
        "                quotes_size.append(size)\n",
        "                #Add the size of the quote in our new file\n",
        "                instance_out['size'] = size \n",
        "                d_file.write((json.dumps(instance_out)+'\\n').encode('utf-8')) # writing in the new file\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kpooZQ3s3od"
      },
      "source": [
        "## First step: pre-process the quotation\n",
        "\n",
        "  Indeed, since the whole analysis for this project is based on the quotations so it is important to keep only the quotations with comparable size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHMv0FD0Eh_W"
      },
      "outputs": [],
      "source": [
        "#See the distribution of the quotation length:\n",
        "sns.displot(data= quotes_size, kde = True, height=8, aspect=1.6) \n",
        "sns.set_style('white')\n",
        "plt.xlabel('Number of word in the quote')\n",
        "plt.title(\"Distribution of the number of word in the different quote\", x = 0.5, y = 1, fontsize = 15)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vOPobprkF2h"
      },
      "source": [
        "According to the distribution, the majority of the quotes are at most 100-words-long. In order to make sure that we analyse complete sentences, we are only going to keep sentences in a range 5 to 100 words long.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DzdyDHiwL9j"
      },
      "outputs": [],
      "source": [
        "#According to the distribution of the number of word in the quotes, we can select only the quotes containg between 5 and 100 words to make sure that we analyse at least one complete sentence.\n",
        "\n",
        "path_to_file = '/content/drive/MyDrive/ADAprojet2021/quotes-2020-clean.json.bz2'\n",
        "path_to_out= '/content/drive/MyDrive/ADAprojet2021/quotes-2020-processed.json.bz2'\n",
        "quotes_size1 =[]\n",
        "\n",
        "with bz2.open(path_to_out, 'wb') as d_file:\n",
        "    with bz2.open(path_to_file, 'rb') as s_file:\n",
        "        for instance in s_file:\n",
        "            instance = json.loads(instance) \n",
        "\n",
        "            if (5<= instance['size'] <= 100):\n",
        "              #We only copy the line associated with the qotation size between 5 and 100:\n",
        "                instance_out = instance\n",
        "\n",
        "                size = len(instance_out['quotation'].split())\n",
        "                quotes_size1.append(size)\n",
        "                d_file.write((json.dumps(instance_out)+'\\n').encode('utf-8'))\n",
        "\n",
        "#Q75 = np.array(quotes_size).quantile(0.75)\n",
        "#print(Q75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnzapNhNP6N_"
      },
      "outputs": [],
      "source": [
        "sns.displot(data= quotes_size1, kde = True, height=8, aspect=1.6) \n",
        "sns.set_style('white')\n",
        "plt.xlabel('Number of word in the quote')\n",
        "plt.title(\"Distribution of the number of word in the different quote\", x = 0.5, y = 1, fontsize = 15)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmVrSNIMqNFd"
      },
      "source": [
        "## Second step : Pre-process the speakers attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3FnKKxRpiOw"
      },
      "source": [
        "During this analysis we are going to use some attributes of the speakers such as their gender, their qids ,their nationality and their US congress bio ID, if there is one.\n",
        "\n",
        "These extracted attributes will be save in a new file.\n",
        "\n",
        "The qIDs are kept so they will allow us to navigate between the quote file and the speaker file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGmQ7vkHvZdB"
      },
      "outputs": [],
      "source": [
        "#This function converts date of birth into datetime format, and replaces \n",
        "#by np.nan if speaker was born before 1900\n",
        "# According to the fact that we are performing a linguistic study, it is essential to make sure that the english spoken is fairily the same. \n",
        "#To do so, we decided to select the speakers of interest regarding the date of birth. \n",
        "#Such processing makes sure that the speakers are contemporary and may speak similarily.\n",
        "def dateofbirth(date):\n",
        "  #Remove speakers who are born before JC\n",
        "    if date[0] == '-' or date[1:5] == '0000': \n",
        "        date = np.nan\n",
        "    else:\n",
        "      #Notice here that we are only focusing on the year, this line will set the every birth days and months to first of january\n",
        "        date = datetime.strptime(date[1:5], '%Y')\n",
        "        if (date.year < 1900):\n",
        "            date = np.nan\n",
        "    return date\n",
        "\n",
        "#This function removes QIDS found in the sorted data that are irrelevant for the analysis\n",
        "def gender(id):\n",
        "    #remove erkek (which is a last name), homosexuality \n",
        "    if id == 'Q106299064' or id == 'Q6636':\n",
        "        return np.nan\n",
        "    #replace male organism, cis male by male\n",
        "    if id == 'Q44148' or id == 'Q15145778':\n",
        "        return 'Q6581097'\n",
        "    #same for female\n",
        "    if id== 'Q43445' or id == 'Q15145779':\n",
        "        return 'Q6581072'\n",
        "    else :\n",
        "        return id\n",
        "\n",
        "#function returns true if us citizen\n",
        "def us_to_bool(nat):\n",
        "    if isinstance(nat, np.ndarray):\n",
        "        return np.isin('Q30', nat)\n",
        "    else : return False\n",
        "\n",
        "#function returns true if in congress\n",
        "def congress(id):\n",
        "    if isinstance(id, str): return True\n",
        "    else : return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bcd47KqGK0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#files = pathlib.Path('./parquet').glob('part-*-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet')\n",
        "files = pathlib.Path(\"/content/drive/MyDrive/ADAprojet2021/Project datasets/speaker_attributes.parquet/\").glob(\"part-*-0d587965-3d8f-41ce-9771-5b8c9024dce9-c000.snappy.parquet\")\n",
        "df = pd.DataFrame()\n",
        "columns = ['id', 'label', 'gender','date_of_birth', 'nationality', 'US_congress_bio_ID']\n",
        "for path in files:\n",
        "    df = pd.concat([df, pd.read_parquet(path, columns = columns)], join='outer', ignore_index=True)\n",
        "    df = df.drop_duplicates(subset='id', keep = False)\n",
        "\n",
        "df.gender = df.gender.apply(lambda x: rem_mult(x))\n",
        "df.gender = df.gender.apply(lambda x: gender(x))\n",
        "\n",
        "df.date_of_birth = df.date_of_birth.apply(lambda x: rem_mult(x))\n",
        "\n",
        "df = df.dropna(subset = ['id', 'gender','date_of_birth'])\n",
        "df = df.astype({'gender': 'category'})\n",
        "\n",
        "df.date_of_birth = df.date_of_birth.apply(lambda x: dateofbirth(x))\n",
        "df = df.dropna(subset = ['date_of_birth'])\n",
        "\n",
        "df.nationality = df.nationality.apply(lambda x: us_to_bool(x))\n",
        "df.US_congress_bio_ID = df.US_congress_bio_ID.apply(lambda x: congress(x))\n",
        "\n",
        "\n",
        "display(df)\n",
        "print(df.memory_usage(deep=True))\n",
        "genders = df.gender.unique()\n",
        "\n",
        "\n",
        "path_to_out = '/content/drive/MyDrive/ADAprojet2021/speaker_attributes_processed.json.bz2'\n",
        "df.to_json(path_to_out, orient = 'records', compression = 'bz2') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qerKaZwczsKt"
      },
      "source": [
        "Since we are doing an observationnal study, it is important to have a great notion of the distribution according to the different features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCDzJmXXlh_X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(50,20))\n",
        "sns.histplot(df.gender)\n",
        "plt.title(\"Histogram showing the distribution of gender \", fontsize=30)\n",
        "plt.xlabel(\"Gender\", fontsize=25)\n",
        "plt.ylabel(\"Numbers of individuals\", fontsize=25)\n",
        "plt.yscale('log')\n",
        "labels = ['transgender female', 'intersex', 'neutrois', 'genderqueer', 'eunuch', 'genderfluid', 'transgender person', 'shemale',\n",
        "          'transgender male', 'transmasculine', 'two-spirit', 'muxe', 'non-binary', 'third gender', 'agender', 'neutral sex',\n",
        "         'female', 'male', 'pangender', 'khatoey', 'bigender', 'demiboy', 'X-gender']\n",
        "plt.xticks(ticks = np.arange(23), labels = labels, rotation=45, fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqAnNMuOweoW"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = []\n",
        "for i in df.date_of_birth:\n",
        "  x.append(i.year)\n",
        "\n",
        "sns.displot(data=x, kde = True, height=8, aspect=1.6) \n",
        "sns.set_style('white')\n",
        "plt.xlabel('Year of birth')\n",
        "plt.xlim(1900,2021)\n",
        "plt.title(\"Distribution of the birth years\", x = 0.5, y = 1, fontsize = 15)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhEbKWw75LC"
      },
      "source": [
        "## Joinining both datasets\n",
        "In this step, we remove quotes by speakers we removed from the speaker dataframe (e.g. because we didn't know the gender), and we cound the number of quotes per speaker, so as to remove speakers that don't have quotes assigned. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZKM7gk175LC"
      },
      "outputs": [],
      "source": [
        "#create a file merging the quotes with the speaker information, to remove quotes by speakers of which we did not have sufficient info\n",
        "path_to_file = '/content/drive/MyDrive/ADAprojet2021/quotes-2020-processed.json.bz2' \n",
        "path_to_pickle = '/content/picklefile.pkl'\n",
        "\n",
        "\n",
        "df_reader = pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=100000, orient = 'records')\n",
        "\n",
        "nbr_chunk = 0\n",
        "\n",
        "df['quote_nbr'] = np.zeros(df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOwwzSof75LC"
      },
      "outputs": [],
      "source": [
        "#function to turn the qids from the quotebank file from list to string format\n",
        "def list_to_first(qids):\n",
        "  return qids[0]\n",
        "\n",
        "with bz2.open(path_to_pickle, 'ab') as pickle_file: #attention ca append au fichier du coup supprimer le fichier avant de relancer\n",
        "    for chunk in df_reader:\n",
        "        chunk.qids = chunk.qids.apply(lambda x: list_to_first(x))\n",
        "        chunk = chunk[(chunk.qids).isin(df.id.tolist())]\n",
        "        occurences = chunk.qids.value_counts()\n",
        "        occurences = pd.DataFrame(occurences)\n",
        "        occurences['id'] = occurences.index.to_series()\n",
        "        occurences.columns = ['quote_nbr', 'id']\n",
        "        df = pd.merge(df, occurences, on='id', how='left' )\n",
        "        df['quote_nbr'] = df.pop('quote_nbr_x')+ df.pop('quote_nbr_y')\n",
        "        nbr_chunk = nbr_chunk + 1\n",
        "        pickle.dump(chunk, pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZMkIKi4y2y"
      },
      "source": [
        "## Natural language processing:\n",
        "\n",
        "Preprocessing of the citations: Put them in the right format, extract the tense of the citation to properly assign the right score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEXXUZjT4yN7"
      },
      "outputs": [],
      "source": [
        "#NLP libraries\n",
        "import spacy, nltk, gensim, sklearn\n",
        "from nltk import pos_tag, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# je sais pas si ca va etre utile mais je le laisse pour le moment\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01M3gjtV5Nmn"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/ADAprojet2021/quotes-2020-processed.json.bz2'\n",
        "quotes = pd.DataFrame(columns=('qids','quotation','verbs' 'tense', 'score'))\n",
        "\n",
        "#create an array containing the tags associated with the tense \n",
        "present = [\"VBP\", \"VBZ\",\"VBG\"]\n",
        "past = [\"VBD\", \"VBN\"]\n",
        "\n",
        "with bz2.open(path, 'rb') as s_file:\n",
        "  for instance in s_file:\n",
        "    tense = ['None']\n",
        "    instance = json.loads(instance) \n",
        "    text = nltk.word_tokenize(instance['quotation'])\n",
        "    tags = nltk.pos_tag(text)\n",
        "\n",
        "    for word,pos in tags:\n",
        "      tense = [pos for word,pos in tags if np.isin(pos, past + present)]\n",
        "  #Check if every verbs are conjugated at the same tense, and assigning the tense to the quotes\n",
        "    if set(tense).issubset(set(present)) == True:\n",
        "      tense = 'present'\n",
        "    if set(tense).issubset(set(past)) == True:\n",
        "      tense = 'past'\n",
        "\n",
        "    quotes = quotes.append({'qids':instance['qids'], 'quotation':instance['quotation'], 'tense': tense}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9y1BgxJkwU"
      },
      "source": [
        "Expression that we want to target in the citations and their associated score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "UOKsSId6Jj4x",
        "outputId": "90f8ff32-a182-44f3-931c-2d3d7895d1e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i'm absolutely certain</td>\n",
              "      <td>0.944286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'm positive</td>\n",
              "      <td>0.938571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm certain</td>\n",
              "      <td>0.935714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i know for a fact</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i know</td>\n",
              "      <td>0.921429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i'm confident</td>\n",
              "      <td>0.918571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i have no doubt</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>i'm sure</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i have no doubt, i mean i'm sure</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i'm fairly confident</td>\n",
              "      <td>0.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>i remember</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>i believe</td>\n",
              "      <td>0.694286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>i would say</td>\n",
              "      <td>0.671429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>i suspect</td>\n",
              "      <td>0.668571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>i could be mistaken but i'm sure</td>\n",
              "      <td>0.668571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>i think</td>\n",
              "      <td>0.665714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>i'm not completely confident, but i think</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>i can't say for sure, but</td>\n",
              "      <td>0.594286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>i think ... but i can't be sure</td>\n",
              "      <td>0.594286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>i'm not certain but</td>\n",
              "      <td>0.591429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>i'm not sure but</td>\n",
              "      <td>0.548571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>... i think</td>\n",
              "      <td>0.535714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>i guess</td>\n",
              "      <td>0.535714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>i could be wrong, but i think</td>\n",
              "      <td>0.525714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>i think, i think</td>\n",
              "      <td>0.515714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>i think ... isn't it</td>\n",
              "      <td>0.497143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>i'm guessing, but i would say</td>\n",
              "      <td>0.484286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>i suppose</td>\n",
              "      <td>0.477143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>oh, i don't know, i suppose</td>\n",
              "      <td>0.431429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>i'm not sure, it's kind of ...</td>\n",
              "      <td>0.415714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     sentence     score\n",
              "0                      i'm absolutely certain  0.944286\n",
              "1                                i'm positive  0.938571\n",
              "2                                 i'm certain  0.935714\n",
              "3                           i know for a fact  0.928571\n",
              "4                                      i know  0.921429\n",
              "5                               i'm confident  0.918571\n",
              "6                             i have no doubt  0.900000\n",
              "7                                    i'm sure  0.860000\n",
              "8            i have no doubt, i mean i'm sure  0.850000\n",
              "9                        i'm fairly confident  0.760000\n",
              "10                                 i remember  0.740000\n",
              "11                                  i believe  0.694286\n",
              "12                                i would say  0.671429\n",
              "13                                  i suspect  0.668571\n",
              "14           i could be mistaken but i'm sure  0.668571\n",
              "15                                    i think  0.665714\n",
              "16  i'm not completely confident, but i think  0.600000\n",
              "17                  i can't say for sure, but  0.594286\n",
              "18            i think ... but i can't be sure  0.594286\n",
              "19                        i'm not certain but  0.591429\n",
              "20                           i'm not sure but  0.548571\n",
              "21                                ... i think  0.535714\n",
              "22                                    i guess  0.535714\n",
              "23              i could be wrong, but i think  0.525714\n",
              "24                           i think, i think  0.515714\n",
              "25                       i think ... isn't it  0.497143\n",
              "26              i'm guessing, but i would say  0.484286\n",
              "27                                  i suppose  0.477143\n",
              "28                oh, i don't know, i suppose  0.431429\n",
              "29             i'm not sure, it's kind of ...  0.415714"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#sentences and associated scores to be used as starting point (taken from the Wesson & Pulford paper)\n",
        "sentences = pd.DataFrame(columns = ['sentence', 'score'])\n",
        "\n",
        "#il manque les expressions au passé\n",
        "#We also removed phrases that were in two parts (I think ... isn't it, I think ... but I can't be sure)\n",
        "sentences.sentence = [\"I'm absolutely certain\", \"I'm positive\", \"I'm certain\", \"I know for a fact\", \"I know\", \"I'm confident\", \"I have no doubt\", \"I'm sure\", \"I have no doubt, I mean I'm sure\", \"I'm fairly confident\", \"I remember\", \"I believe\", \"I would say\", \"I suspect\", \"I could be mistaken but I'm sure\", \"I think\", \"I'm not completely confident, but I think\", \"I can't say for sure, but\", \"I think ... but I can't be sure\", \"I'm not certain but\",\"I'm not sure but\",\"... I think\", \"I guess\", \"I could be wrong, but I think\", \"I think, I think\", \"I think ... isn't it\", \"I'm guessing, but I would say\", \"I suppose\", \"Oh, I don't know, I suppose\", \"I'm not sure, it's kind of ...\"]\n",
        "\n",
        "#score in the article follow uk grading system (0-7, 7 being the best). \n",
        "sentences.score = [6.61, 6.57, 6.55, 6.5, 6.45, 6.43, 6.3, 6.02, 5.95, 5.32, 5.18, 4.86, 4.7, 4.68, 4.68, 4.66, 4.2, 4.16, 4.16, 4.14, 3.84, 3.75, 3.75, 3.68, 3.61, 3.48, 3.39, 3.34, 3.02, 2.91]\n",
        "\n",
        "#We change this to a 0-1 scale, as it is more intuitive. \n",
        "sentences.score = sentences['score'].apply(lambda x: x/7)\n",
        "\n",
        "#to enable proper comparison we need to case fold the expressions to avoid a missing\n",
        "sentences.sentence = sentences['sentence'].apply(lambda x: x.casefold())\n",
        "\n",
        "display(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je laisse la cellule qui genere un data frame test si quelqu'un a besoin d'un sous ensemble pour tester ses trucs"
      ],
      "metadata": {
        "id": "1thY2CFI9kD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKBHW1vn66Z8"
      },
      "outputs": [],
      "source": [
        "#Extract a small part of the whole file to test \n",
        "path = '/content/drive/MyDrive/ADAprojet2021/quotes-2020-processed.json.bz2'\n",
        "test = pd.DataFrame(columns=('qids','quotation', 'findings', 'score'))\n",
        "i = 0\n",
        "with bz2.open(path, 'rb') as s_file:\n",
        "  for instance in s_file:\n",
        "    instance = json.loads(instance) \n",
        "    i = i + 1\n",
        "    test = test.append({'qids': instance['qids'], 'quotation': instance['quotation']}, ignore_index=True)\n",
        "    if i == 15:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzEBUqBTF5IU"
      },
      "source": [
        "Find the expressions of confidence in the quotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "8_ndfm0aCi2B",
        "outputId": "b78cf38e-9c56-4774-8276-dd79360f7083"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qids</th>\n",
              "      <th>quotation</th>\n",
              "      <th>findings</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Q367796]</td>\n",
              "      <td>department of homeland security was livid and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Q20684375]</td>\n",
              "      <td>i met them when they just turned 4 and 7. they...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Q5268447]</td>\n",
              "      <td>the delay will have an impact on slough but th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Q4864119]</td>\n",
              "      <td>the scheme treats addiction as an illness and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Q816459]</td>\n",
              "      <td>these actions will allow households who have a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[Q30164281]</td>\n",
              "      <td>1. fm is entitled to go straight to press conf...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[Q56255401]</td>\n",
              "      <td>11 straight weeks of pre-season,</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[Q26923564]</td>\n",
              "      <td>2019 was a landmark year for fiverr as we comp...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[Q4749380]</td>\n",
              "      <td>7pm is when most hospitals change shifts. that...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[Q970000]</td>\n",
              "      <td>a city is not an accident but the result of co...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[Q57019294]</td>\n",
              "      <td>a detailed description of ogre, the olg model</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[Q6379626]</td>\n",
              "      <td>a face-to-face duty lawyer service provided by...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[Q771586]</td>\n",
              "      <td>a few of the candidates who will do better in ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[Q4909730]</td>\n",
              "      <td>a full restoration can run from about 120 all ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[Q16731415]</td>\n",
              "      <td>a host of other protections</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           qids  ... score\n",
              "0     [Q367796]  ...   NaN\n",
              "1   [Q20684375]  ...   NaN\n",
              "2    [Q5268447]  ...   NaN\n",
              "3    [Q4864119]  ...   NaN\n",
              "4     [Q816459]  ...   NaN\n",
              "5   [Q30164281]  ...   NaN\n",
              "6   [Q56255401]  ...   NaN\n",
              "7   [Q26923564]  ...   NaN\n",
              "8    [Q4749380]  ...   NaN\n",
              "9     [Q970000]  ...   NaN\n",
              "10  [Q57019294]  ...   NaN\n",
              "11   [Q6379626]  ...   NaN\n",
              "12    [Q771586]  ...   NaN\n",
              "13   [Q4909730]  ...   NaN\n",
              "14  [Q16731415]  ...   NaN\n",
              "\n",
              "[15 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from nltk.text import Text\n",
        "import re \n",
        "\n",
        "#to enable proper comparison we need to case fold the citations to avoid a missing\n",
        "\n",
        "#quotes['quotation']= quotes['quotation'].apply(lambda x: x.casefold())\n",
        "\n",
        "test['quotation']= test['quotation'].apply(lambda x: x.casefold())\n",
        "\n",
        "display(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#a mettre plus tot du coup \n",
        "#Apparemment le warning c'est un conniere de desaccord entre numpy et native python du coup je vais juste les ingnorer avec les lignes suivante :\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ],
      "metadata": {
        "id": "vUz7qaMX9wqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "UZa0bl0-JPnF",
        "outputId": "fa8dc9d1-c583-4635-e47e-0f59ceb6cde7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qids</th>\n",
              "      <th>quotation</th>\n",
              "      <th>findings</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Q367796]</td>\n",
              "      <td>department of homeland security was livid and ...</td>\n",
              "      <td>[of, the]</td>\n",
              "      <td>[4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Q20684375]</td>\n",
              "      <td>i met them when they just turned 4 and 7. they...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Q5268447]</td>\n",
              "      <td>the delay will have an impact on slough but th...</td>\n",
              "      <td>[the]</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Q4864119]</td>\n",
              "      <td>the scheme treats addiction as an illness and ...</td>\n",
              "      <td>[the]</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Q816459]</td>\n",
              "      <td>these actions will allow households who have a...</td>\n",
              "      <td>[of, the]</td>\n",
              "      <td>[4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[Q30164281]</td>\n",
              "      <td>1. fm is entitled to go straight to press conf...</td>\n",
              "      <td>[is, not]</td>\n",
              "      <td>[1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[Q56255401]</td>\n",
              "      <td>11 straight weeks of pre-season,</td>\n",
              "      <td>[of]</td>\n",
              "      <td>[5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[Q26923564]</td>\n",
              "      <td>2019 was a landmark year for fiverr as we comp...</td>\n",
              "      <td>[the]</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[Q4749380]</td>\n",
              "      <td>7pm is when most hospitals change shifts. that...</td>\n",
              "      <td>[is, why]</td>\n",
              "      <td>[1, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[Q970000]</td>\n",
              "      <td>a city is not an accident but the result of co...</td>\n",
              "      <td>[is, not, of, the]</td>\n",
              "      <td>[1, 2, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[Q57019294]</td>\n",
              "      <td>a detailed description of ogre, the olg model</td>\n",
              "      <td>[of, the]</td>\n",
              "      <td>[4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[Q6379626]</td>\n",
              "      <td>a face-to-face duty lawyer service provided by...</td>\n",
              "      <td>[is, not]</td>\n",
              "      <td>[1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[Q771586]</td>\n",
              "      <td>a few of the candidates who will do better in ...</td>\n",
              "      <td>[of, the]</td>\n",
              "      <td>[4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[Q4909730]</td>\n",
              "      <td>a full restoration can run from about 120 all ...</td>\n",
              "      <td>[the]</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[Q16731415]</td>\n",
              "      <td>a host of other protections</td>\n",
              "      <td>[of]</td>\n",
              "      <td>[5]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           qids  ...         score\n",
              "0     [Q367796]  ...        [4, 5]\n",
              "1   [Q20684375]  ...            []\n",
              "2    [Q5268447]  ...           [4]\n",
              "3    [Q4864119]  ...           [4]\n",
              "4     [Q816459]  ...        [4, 5]\n",
              "5   [Q30164281]  ...        [1, 2]\n",
              "6   [Q56255401]  ...           [5]\n",
              "7   [Q26923564]  ...           [4]\n",
              "8    [Q4749380]  ...        [1, 3]\n",
              "9     [Q970000]  ...  [1, 2, 4, 5]\n",
              "10  [Q57019294]  ...        [4, 5]\n",
              "11   [Q6379626]  ...        [1, 2]\n",
              "12    [Q771586]  ...        [4, 5]\n",
              "13   [Q4909730]  ...           [4]\n",
              "14  [Q16731415]  ...           [5]\n",
              "\n",
              "[15 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#put the different expression in a format enable multiple comparison:\n",
        "rx = r\"(?=\\b({})\\b)\".format(\"|\".join(map(re.escape, sorted(sentences.sentence, key=len, reverse=True))))\n",
        "pattern = re.compile(rx)\n",
        "\n",
        "#keep the expressions found in an array to allow score assessing:\n",
        "#quotes['findings'] = quotes['quotation'].apply(lambda x: np.unique(re.findall(pattern, x)))\n",
        "test['findings'] = test['quotation'].apply(lambda x: np.unique(re.findall(pattern, x)))\n",
        "\n",
        "#findings contains string or ??, need to figure out might be a problem\n",
        "\n",
        "#score assessing:\n",
        "#quotes['score'] = quotes['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences['sentence'][ind] in x)])\n",
        "test['score'] = test['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences['sentence'][ind] in x)])\n",
        "#display(quotes)\n",
        "display(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some expressions take into account the tense of the sentences, hence we create a new column that indicates the tense in order to properly assess the score "
      ],
      "metadata": {
        "id": "NKjqSI-t93Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create an array containing the tags associated with the tense \n",
        "present = [\"VBP\", \"VBZ\",\"VBG\"]\n",
        "past = [\"VBD\", \"VBN\"]\n",
        "\n",
        "#quotes['tags'] = quotes['quotations'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
        "test['tags'] = test['quotation'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
        "\n",
        "#quotes['tense'] = quotes['tags'].apply(lambda x: [pos for word,pos in tags if np.isin(pos, past + present)])\n",
        "test['tense'] = test['tags'].apply(lambda x: [pos for word,pos in x if np.isin(pos, past + present)])\n",
        "\n",
        "#quotes['tense'] = quotes['tense'].apply(lambda x: 'present' if set(x).issubset(set(present)) == True else 'past' )\n",
        "test['tense'] = test['tense'].apply(lambda x: 'present' if set(x).issubset(set(present)) == True else 'past')\n",
        "\n",
        "#quotes['score'] = quotes['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences['sentence'][ind] in x)])\n",
        "\n",
        "# dans le futur on aura besoin de faire une difeerenciation entre les citations qui sont au passé et au present pour associer le bon score:\n",
        "#quotes[quotes.tense == 'present']['score'] = quotes[quotes.tense == 'present']['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences[sentences.tense == 'present']['sentence'][ind] in x)])\n",
        "#quotes[quotes.tense == 'past']['score'] = quotes[quotes.tense == 'past']['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences[sentences.tense == 'past']['sentence'][ind] in x)])\n",
        "\n",
        "test['score'] = test['findings'].apply(lambda x: [sentences['score'][ind] for ind in sentences.index if(sentences['sentence'][ind] in x)])\n",
        "display(test)"
      ],
      "metadata": {
        "id": "cZiz8wix91ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcltWgpGoXe"
      },
      "source": [
        "# CODE DE FLORETTE:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9JsuJQyGnyb"
      },
      "outputs": [],
      "source": [
        "#si besoin\n",
        "#pip install -U sentence-transformers\n",
        "\n",
        "#test de comparaison semantique avec des trucs randoms\n",
        "\n",
        "sentences = [\n",
        "    \"Three years later, the coffin was still full of Jello.\",\n",
        "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
        "    \"The person box was packed with jelly many dozens of months later.\",\n",
        "    \"He found a leprechaun in his walnut shell.\"\n",
        "]\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\pretrained_model') #'bert-base-nli-mean-tokens'\n",
        "\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(\n",
        "    [sentence_embeddings[0]],\n",
        "    sentence_embeddings[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "eFNAId2eGu5e",
        "outputId": "23303a31-c829-4b53-e6ec-36219a12d7fa"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          },
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ded951e85716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#note: normalement pas besoin d'avoir le model sur l'ordi, il suffit de mettre le nom 'all-MiniLM-L6-v2' et SentenceTransformer va le chercher tout seul sur internet, mais là il y a un bug genre ajd,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#donc j'ai téléchargé le model sur mon ordi et mis le path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\pretrained_model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#with model all-MiniLM-L6-v2' # or use 'bert-base-nli-mean-tokens'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#test de comparaison semantique de quelques une de nos citations avec une des phrases types du paper\n",
        "#j'ai fait comme dans un exemple de BERT que j'ai trouvé sur internet. j'ai pris le pretrained model all-MiniLM-L6-v2\n",
        "\n",
        "\n",
        "#note: normalement pas besoin d'avoir le model sur l'ordi, il suffit de mettre le nom 'all-MiniLM-L6-v2' et SentenceTransformer va le chercher tout seul sur internet, mais là il y a un bug genre ajd, \n",
        "#donc j'ai téléchargé le model sur mon ordi et mis le path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\pretrained_model') #with model all-MiniLM-L6-v2' # or use 'bert-base-nli-mean-tokens'\n",
        "\n",
        "\n",
        "#Extract a small part of the whole file to test \n",
        "path = r'C:\\Users\\flore\\Desktop\\EPFL\\MA1\\ada\\PROJET\\quotes-2020-precessed.json.bz2' #path de mon ordi\n",
        "test = pd.DataFrame(columns=('qids','quotation'))\n",
        "i = 0\n",
        "with bz2.open(path, 'rb') as s_file:\n",
        "  for instance in s_file:\n",
        "    instance = json.loads(instance) \n",
        "    print(instance)\n",
        "    #print(instance['quotation'])\n",
        "    i = i + 1\n",
        "    test = test.append({'quoteID': instance['quoteID'], 'quotation': instance['quotation']}, ignore_index=True)\n",
        "    #print(i)\n",
        "    if i == 10:\n",
        "      break\n",
        "\n",
        "#embedding des 10 citations\n",
        "sentence_embeddings = model.encode(test['quotation'])\n",
        "\n",
        "#comparaison avec une des phrase du paper: cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "to_compare = [\"I'm sure it's\"]\n",
        "to_compare_embedding = model.encode(to_compare)\n",
        "\n",
        "cosine_similarity(\n",
        "    [to_compare_embedding[0]],\n",
        "    sentence_embeddings[0:]\n",
        ")\n",
        "\n",
        "#l'output ets un array des coefficients de similarité (là ils osnt tous ba vu qu'aucune citation n'a un truc prche de I'm sure it's)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knH5ZFniGywc"
      },
      "outputs": [],
      "source": [
        "#autre test avec le début de citation pour voir si il donne bien un score plus élevé pour cette citation\n",
        "to_compare_2 = [\"Department of Homeland Security was livid and\"]\n",
        "to_compare_2_embedding = model.encode(to_compare_2)\n",
        "\n",
        "cosine_similarity(\n",
        "    [to_compare_2_embedding[0]],\n",
        "    sentence_embeddings[0:]\n",
        ")\n",
        "#le premier est bien plus élevé"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project_2_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}